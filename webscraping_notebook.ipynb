{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import webscraping\n",
        "import pandas as pd\n",
        "import fitz\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "#Here is where we set the keyword list to be used across all sources\n",
        "#This will be used as a list of search terms to look through in order to find sustainability-related headlines\n",
        "keywords = [\n",
        "    \"sustainability\",\n",
        "    \"sustainable\",\n",
        "    \"greenwashing\",\n",
        "    \"carbon-neutral\",\n",
        "    \"renewable\",\n",
        "    \"eco-friendly\",\n",
        "    \"pollution\",\n",
        "    \"emissions\",\n",
        "    \"recycling\",\n",
        "    \"green\",\n",
        "    \"cruelty-free\",\n",
        "    \"zero-waste\",\n",
        "    \"carbon\",\n",
        "    \"energy\",\n",
        "    \"climate\",\n",
        "    \"solar\",\n",
        "    \"power\",\n",
        "    \"water\",\n",
        "    \"wind\",\n",
        "    \"waste\",\n",
        "    \"gas\",\n",
        "    \"electricity\",\n",
        "    \"fossil\",\n",
        "    \"clean\",\n",
        "    \"renewable energy\",\n",
        "    \"climate change\",\n",
        "    \"fossil fuels\",\n",
        "    \"green energy\",\n",
        "    \"solar farm\",\n",
        "    \"greenhouse gas\",\n",
        "    \"clean energy\",\n",
        "    \"sustainable consumer\",\n",
        "    \"sustainable developement\",\n",
        "    \"sustainable consumption\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "dhCKdndlRZpk",
        "outputId": "72803d11-5c26-476f-ed1e-5665e5b6440a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 107 (webscraping.py, line 108)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-312267109.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<cell line: 0>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import webscraping\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/content/webscraping.py\"\u001b[0;36m, line \u001b[0;32m108\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Journals\n",
        "\n",
        "#get pdfs from folder on your computer, you will need to change this when running the code for yourself\n",
        "location = '/Users/famema17/Downloads/ucla25-26/fall25/pic16B/project/data/'\n",
        "\n",
        "#this code uses the funtion pdf_scraper from webscraping.py to search these journals for the keywords\n",
        "\n",
        "pdf_path = location + 'anneJournal.pdf'\n",
        "df = pdf_scraper(pdf_path, keywords)\n",
        "df\n",
        "\n",
        "pdf_path2 = location + 'andersonJournal.pdf'\n",
        "df2 = pdf_scraper(pdf_path2, keywords)\n",
        "df2\n",
        "\n",
        "pdf_path3 = location + 'zhangJournal.pdf'\n",
        "df3 = pdf_scraper(pdf_path3, keywords)\n",
        "df3\n",
        "\n",
        "pdf_path4 = location + 'carpenterJournal.pdf'\n",
        "df4 = pdf_scraper(pdf_path4, keywords)\n",
        "df4\n",
        "\n",
        "pdf_path5 = location + 'horichJournal.pdf'\n",
        "df5 = pdf_scraper(pdf_path5, keywords)\n",
        "df5\n",
        "\n",
        "pdf_path6 = location +'loresJournal.pdf'\n",
        "df6 = pdf_scraper(pdf_path6, keywords)\n",
        "df6\n",
        "\n",
        "pdf_path7 = location + 'shettyJournal.pdf'\n",
        "df7 = pdf_scraper(pdf_path7, keywords)\n",
        "df7\n",
        "\n",
        "pdf_path8 = location + 'veraJournal.pdf'\n",
        "df8 = pdf_scraper(pdf_path8, keywords)\n",
        "df8\n",
        "\n",
        "\n",
        "#combine the individual data into one dataframe\n",
        "df_journals = journalpdf_concat(df, df2, df3, df4, df5, df6, df7,df8)\n",
        "df_journals.head()"
      ],
      "metadata": {
        "id": "9OP0ef5FTxAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#News Articles\n",
        "\n",
        "#ap news\n",
        "dfs_with_all_keywords = pd.DataFrame()\n",
        "for word in keywords:\n",
        "    print(\"word: \" + word)\n",
        "    url = \"https://apnews.com/search?q=\" + word + \"%20&s=0&p=\"\n",
        "    df_per_keyword = news_webscrapper(word, url, \"div\", \"PagePromo\", \"div\", \"PagePromo-description\", \"div\", \"PagePromo-title\")\n",
        "    dfs_with_all_keywords = pd.concat([dfs_with_all_keywords, df_per_keyword], ignore_index=True) #index thing makes everything combine well\n",
        "    #now add a column that mentions news: AP news\n",
        "    dfs_with_all_keywords[\"Source Type\"] = \"News: AP News\"\n",
        "\n",
        "\n",
        "#dfs_with_all_keywords #filter with specfic words\n",
        "df_ap = dfs_with_all_keywords\n",
        "\n",
        "#bbc news\n",
        "dfs_with_all_keywords = pd.DataFrame()\n",
        "for word in keywords:\n",
        "    print(\"word: \" + word)\n",
        "    url = \"https://www.bbc.com/search?q=\" + str(word) + \"&page=\"\n",
        "    df_per_keyword = news_webscrapper(word, url, \"div\", \"sc-cdecfb63-0 cJcHVD\", \"h2\", \"sc-fa814188-3 ifRknM\", \"div\", \"sc-cdecfb63-3 iIFdam\")\n",
        "    dfs_with_all_keywords = pd.concat([dfs_with_all_keywords, df_per_keyword], ignore_index=True) #index thing makes everything combine well\n",
        "    #now add a column that mentions news: BBC news\n",
        "    dfs_with_all_keywords[\"Source Type\"] = \"News: BBC News\"\n",
        "\n",
        "\n",
        "#dfs_with_all_keywords #filter with specfic words\n",
        "df_bbc = dfs_with_all_keywords\n",
        "\n",
        "#pbs news\n",
        "dfs_with_all_keywords = pd.DataFrame()\n",
        "for word in keywords:\n",
        "    print(\"word: \" + word)\n",
        "    url = \"https://www.pbs.org/newshour/search-results?q=\" + word + \"&pnb=\"\n",
        "    df_per_keyword = news_webscrapper(word, url, \"li\", \"search-result\", \"h4\", \"search-result__title\", \"p\", \"search-result__snippet\")\n",
        "    dfs_with_all_keywords = pd.concat([dfs_with_all_keywords, df_per_keyword], ignore_index=True) #index thing makes everything combine well\n",
        "    #now add a column that mentions news: BBC news\n",
        "    dfs_with_all_keywords[\"Source Type\"] = \"News: PBS News\"\n",
        "\n",
        "df_pbs= dfs_with_all_keywords\n",
        "\n",
        "#putting them together\n",
        "df_news = pd.concat([df_ap, df_bbc], ignore_index=True)\n",
        "df_news = pd.concat([df_news, df_pbs], ignore_index=True)\n",
        "df_news.head()"
      ],
      "metadata": {
        "id": "NS_S7lKXRGti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Magazines\n",
        "#WARNING: this section is very slow, try not to run more than is needed\n",
        "#it also is very sensitive to changes, try to make as few edits as possible\n",
        "\n",
        "number_of_clicks = 5 #this is how many times you click the \"more stories\" button\n",
        "#for a quick test of the code, set to 1\n",
        "\n",
        "#Vogue\n",
        "Headlines=[]\n",
        "Bylines=[]\n",
        "Dates=[]\n",
        "Keyword=[]\n",
        "\n",
        "for key in keywords:\n",
        "    keyword = key.upper()\n",
        "    url = \"https://www.vogue.com/search?q=\" + keyword +\"&sort=score+desc\"\n",
        "    soup = link2soup(url)\n",
        "\n",
        "    if soup.find('h1', class_=\"BaseWrap-sc-gzmcOU BaseText-eqOrNE SectionHeaderHed-jvdKHz deqABF iqLDlA dabhPm section-header__hed\").get_text() == 'No stories found for your search':\n",
        "        print(\"oops can't find anything\")\n",
        "    else:\n",
        "        soup = click_button_vogue(url, number_of_clicks) #DO NOT REMOVE, is needed to load page properly for scraping\n",
        "        for headline in soup.find_all('h3'):\n",
        "            Headlines.append(headline.get_text())\n",
        "        for byline in soup.find_all('span', itemprop=\"name\", class_=\"BylineNamesWrapper-jrdaOa fXeqQN\"):\n",
        "            Bylines.append(byline.get_text().replace(\"By \", \"\"))\n",
        "        for date in soup.find_all('time', class_=\"BaseWrap-sc-gzmcOU BaseText-eqOrNE SummaryItemBylinePublishDate-czeIQl deqABF mdLVF gtAwEp summary-item__publish-date\"):\n",
        "            Dates.append(date.get_text())\n",
        "            Keyword.append(key)\n",
        "\n",
        "df = pd.DataFrame({\"Headlines\":Headlines, \"Bylines\":Bylines, \"Dates\":Dates, \"Keyword\":Keyword})\n",
        "df[\"Source\"]=\"Vogue\"\n",
        "df[\"Source Type\"]=\"Magazine\"\n",
        "df_vogue=df\n",
        "\n",
        "#TeenVogue\n",
        "df_teenvogue = teenvogue_scraper(\"teenvogue\", keywords, number_of_clicks,\n",
        "                                  [\"h1\", \"BaseWrap-sc-gzmcOU BaseText-eqOrNE SectionHeaderHed-jvdKHz deqABF ddMSAi hZnCQy section-header__hed\",\n",
        "                                   \"h2\", \"SummaryItemHedBase-hnYOxl fxKPGc summary-item__hed\",\n",
        "                                   \"span\", \"BylineNamesWrapper-jrdaOa fXeqQN\",\n",
        "                                   \"time\", \"BaseWrap-sc-gzmcOU BaseText-eqOrNE SummaryItemBylinePublishDate-czeIQl deqABF eVEhbw gtAwEp summary-item__publish-date\"])"
      ],
      "metadata": {
        "id": "DcX54_mKWeCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_magazines = pd.concat(df_vogue, df_teenvogue)\n",
        "df_magazines.head()"
      ],
      "metadata": {
        "id": "OTa93ynfYgnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining dataframes and saving them as csv files\n",
        "\n",
        "#saving the separate dataframes\n",
        "df_journals.to_csv('journal_data.csv', index=False)\n",
        "df_news.to_csv('news_data.csv', index=False)\n",
        "df_magazines.to_csv('magazine_data.csv', index=False)"
      ],
      "metadata": {
        "id": "bU8gWeqpayXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making one consistant dataframe and saving all of the data\n",
        "#we did not do a great job keeping all of the column names consistant across the different dataframes\n",
        "#so this line will not run\n",
        "#I need you to find the columns that all the dataframes have and rename them so that they all line up\n",
        "#df_renamed = df.rename(columns={'old_col_A': 'new_col_X', 'old_col_B': 'new_col_Y'})\n",
        "\n",
        "df_all_sources = pd.concat([df_journals, df_news, df_magazines], ignore_index=True)\n",
        "df_all_sources.head()\n",
        "df_all_sources.to_csv('webscraped_data.csv', index=False)"
      ],
      "metadata": {
        "id": "0xnJCyrQjD8n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}